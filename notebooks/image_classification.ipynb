{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification with Transfer Learning\n",
    "### This notebook trains a CNN-based classifier to distinguish between cats and dogs.\n",
    "\n",
    "**Dataset:**  \n",
    "The dataset has been imported from Kaggle, the cats and dogs dataset\n",
    "\n",
    "**Approach:**  \n",
    "We use an object-oriented approach with TensorFlow/Keras and transfer learning \n",
    "(using EfficientNetB0) to achieve at least 90% accuracy.\n",
    "\n",
    "**Deliverables:**\n",
    "- Trained model saved as `../models/cnn_model.h5`\n",
    "- This notebook (`image_classification.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'fraud (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/ZAWADI/Envs/fraud/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\ZAWADI\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ZAWADI\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import the required libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models, optimizers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EfficientNetB0\n",
      "File \u001b[1;32mc:\\Users\\ZAWADI\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\ZAWADI\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\ZAWADI\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Define the ImageClassifier Class\n",
    "# \n",
    "# The `ImageClassifier` class encapsulates:\n",
    "# - Loading and preprocessing the data.\n",
    "# - Building the transfer learning model.\n",
    "# - Training, evaluating, and saving the model.\n",
    "# \n",
    "# The final model is saved to the `models` folder.\n",
    "\n",
    "# %% [code]\n",
    "class ImageClassifier:\n",
    "    def __init__(self, data_dir, img_size=(224, 224), batch_size=32, val_split=0.2, seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.seed = seed\n",
    "        self.train_ds = None\n",
    "        self.val_ds = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the data directory, splitting into training and validation sets.\n",
    "        Expects subfolders in data_dir for each class.\n",
    "        \"\"\"\n",
    "        self.train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.data_dir,\n",
    "            validation_split=self.val_split,\n",
    "            subset=\"training\",\n",
    "            seed=self.seed,\n",
    "            image_size=self.img_size,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        self.val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.data_dir,\n",
    "            validation_split=self.val_split,\n",
    "            subset=\"validation\",\n",
    "            seed=self.seed,\n",
    "            image_size=self.img_size,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        # Prefetch for performance\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_ds = self.train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "        self.val_ds = self.val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "        print(\"Data loaded successfully!\")\n",
    "        return self.train_ds, self.val_ds\n",
    "\n",
    "    def build_model(self, fine_tune_at=100):\n",
    "        \"\"\"\n",
    "        Build a transfer learning model using EfficientNetB0.\n",
    "        - Freeze the base model initially.\n",
    "        - Add a global average pooling layer and a dense classifier.\n",
    "        - Optionally, unfreeze part of the base model for fine-tuning.\n",
    "        \"\"\"\n",
    "        base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(*self.img_size, 3))\n",
    "        base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "        # Add classification head\n",
    "        inputs = tf.keras.Input(shape=(*self.img_size, 3))\n",
    "        x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "        self.model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(),\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        print(\"Model built successfully!\")\n",
    "        return self.model\n",
    "\n",
    "    def fine_tune_model(self, fine_tune_at=100):\n",
    "        \"\"\"\n",
    "        Unfreeze the base model from the fine_tune_at layer onward and recompile for fine-tuning.\n",
    "        \"\"\"\n",
    "        base_model = self.model.layers[2]  # EfficientNetB0 is the 3rd layer in our model\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(1e-5),  # lower learning rate for fine-tuning\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        print(\"Model fine-tuning configuration updated!\")\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model with early stopping and model checkpoint callbacks.\n",
    "        Aim for at least 90% validation accuracy.\n",
    "        \"\"\"\n",
    "        # Ensure the models folder exists\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "            ModelCheckpoint(\"models/cnn_model.h5\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "        ]\n",
    "        history = self.model.fit(\n",
    "            self.train_ds,\n",
    "            validation_data=self.val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        print(\"Training complete!\")\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the validation set.\n",
    "        \"\"\"\n",
    "        loss, accuracy = self.model.evaluate(self.val_ds)\n",
    "        print(f\"Validation Loss: {loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        return loss, accuracy\n",
    "\n",
    "    def save_model(self, filepath=\"models/cnn_model.h5\"):\n",
    "        \"\"\"\n",
    "        Save the trained model to the specified filepath.\n",
    "        \"\"\"\n",
    "        self.model.save(filepath)\n",
    "        print(f\"Model saved as {filepath}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training the Model\n",
    "# \n",
    "# We will now:\n",
    "# 1. Load the data from `data/pet_images`\n",
    "# 2. Build the transfer learning model\n",
    "# 3. Train and fine-tune the model until we achieve at least 90% accuracy\n",
    "# 4. Save the trained model in the `models` folder as `models/cnn_model.h5`\n",
    "\n",
    "# Usage\n",
    "# Set the dataset directory\n",
    "data_dir = '../data/pet_images'\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = ImageClassifier(data_dir=data_dir, img_size=(224, 224), batch_size=32)\n",
    "\n",
    "# Load data\n",
    "classifier.load_data()\n",
    "\n",
    "# Build the model\n",
    "classifier.build_model()\n",
    "\n",
    "# Train the model (initial training with frozen base)\n",
    "history = classifier.train_model(epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = classifier.evaluate_model()\n",
    "\n",
    "# If validation accuracy is below 90%, fine-tune the model\n",
    "if accuracy < 0.90:\n",
    "    print(\"Validation accuracy below 90%, fine-tuning the model...\")\n",
    "    classifier.fine_tune_model(fine_tune_at=100)\n",
    "    # Continue training\n",
    "    history_ft = classifier.train_model(epochs=10)\n",
    "    loss, accuracy = classifier.evaluate_model()\n",
    "\n",
    "# Save the model in the models folder\n",
    "classifier.save_model('../models/cnn_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
